# Telegraf Configuration
#
# Telegraf is entirely plugin driven. All metrics are gathered from the
# declared inputs, and sent to the declared outputs.
#
# Plugins must be declared in here to be active.
# To deactivate a plugin, comment out the name and any variables.
#
# Use 'telegraf -config telegraf.conf -test' to see what metrics a config
# file would generate.
#
# Environment variables can be used anywhere in this config file, simply surround
# them with ${}. For strings the variable must be within quotes (ie, "${STR_VAR}"),
# for numbers and booleans they should be plain (ie, ${INT_VAR}, ${BOOL_VAR})


# Global tags can be specified here in key="value" format.
[global_tags]
  lab="${tag}"
  # dc = "us-east-1" # will tag all metrics with dc=us-east-1
  # rack = "1a"
  ## Environment variables can be used as tags, and throughout the config file
  # user = "$USER"

# Configuration for telegraf agent
[agent]
  ## Default data collection interval for all inputs
  interval = "10s"
  ## Rounds collection interval to 'interval'
  ## ie, if interval="10s" then always collect on :00, :10, :20, etc.
  round_interval = true

  ## Telegraf will send metrics to outputs in batches of at most
  ## metric_batch_size metrics.
  ## This controls the size of writes that Telegraf sends to output plugins.
  metric_batch_size = 1000

  ## Maximum number of unwritten metrics per output.  Increasing this value
  ## allows for longer periods of output downtime without dropping metrics at the
  ## cost of higher maximum memory usage.
  metric_buffer_limit = 10000

  ## Collection jitter is used to jitter the collection by a random amount.
  ## Each plugin will sleep for a random time within jitter before collecting.
  ## This can be used to avoid many plugins querying things like sysfs at the
  ## same time, which can have a measurable effect on the system.
  collection_jitter = "0s"

  ## Collection offset is used to shift the collection by the given amount.
  ## This can be be used to avoid many plugins querying constraint devices
  ## at the same time by manually scheduling them in time.
  # collection_offset = "0s"

  ## Default flushing interval for all outputs. Maximum flush_interval will be
  ## flush_interval + flush_jitter
  flush_interval = "10s"
  ## Jitter the flush interval by a random amount. This is primarily to avoid
  ## large write spikes for users running a large number of telegraf instances.
  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s
  flush_jitter = "0s"

  ## Collected metrics are rounded to the precision specified. Precision is
  ## specified as an interval with an integer + unit (e.g. 0s, 10ms, 2us, 4s).
  ## Valid time units are "ns", "us" (or "Âµs"), "ms", "s".
  ##
  ## By default or when set to "0s", precision will be set to the same
  ## timestamp order as the collection interval, with the maximum being 1s:
  ##   ie, when interval = "10s", precision will be "1s"
  ##       when interval = "250ms", precision will be "1ms"
  ##
  ## Precision will NOT be used for service inputs. It is up to each individual
  ## service input to set the timestamp at the appropriate precision.
  precision = "0s"

  ## Log at debug level.
  #debug = true
  ## Log only error level messages.
  # quiet = false

  ## Log format controls the way messages are logged and can be one of "text",
  ## "structured" or, on Windows, "eventlog".
  # logformat = "text"

  ## Message key for structured logs, to override the default of "msg".
  ## Ignored if `logformat` is not "structured".
  # structured_log_message_key = "message"

  ## Name of the file to be logged to or stderr if unset or empty. This
  ## setting is ignored for the "eventlog" format.
  logfile = "${logspath}/telegraf.log"

  ## The logfile will be rotated after the time interval specified.  When set
  ## to 0 no time based rotation is performed.  Logs are rotated only when
  ## written to, if there is no log activity rotation may be delayed.
  # logfile_rotation_interval = "0h"

  ## The logfile will be rotated when it becomes larger than the specified
  ## size.  When set to 0 no size based rotation is performed.
  logfile_rotation_max_size = "10MB"

  ## Maximum number of rotated archives to keep, any older logs are deleted.
  ## If set to -1, no archives are removed.
  logfile_rotation_max_archives = 3

  ## Pick a timezone to use when logging or type 'local' for local time.
  ## Example: America/Chicago
  log_with_timezone = "local"

  ## Override default hostname, if empty use os.Hostname()
  # hostname = ""
  ## If set to true, do no set the "host" tag in the telegraf agent.
  # omit_hostname = false

  ## Method of translating SNMP objects. Can be "netsnmp" (deprecated) which
  ## translates by calling external programs snmptranslate and snmptable,
  ## or "gosmi" which translates using the built-in gosmi library.
  # snmp_translator = "netsnmp"

  ## Name of the file to load the state of plugins from and store the state to.
  ## If uncommented and not empty, this file will be used to save the state of
  ## stateful plugins on termination of Telegraf. If the file exists on start,
  ## the state in the file will be restored for the plugins.
  # statefile = ""

  ## Flag to skip running processors after aggregators
  ## By default, processors are run a second time after aggregators. Changing
  ## this setting to true will skip the second run of processors.
  # skip_processors_after_aggregators = false


###############################################################################
#                            OUTPUT PLUGINS                                   #
###############################################################################


 # Configuration for sending metrics to InfluxDB 2.0
 [[outputs.influxdb_v2]]
   ## The URLs of the InfluxDB cluster nodes.
   ##
   ## Multiple URLs can be specified for a single cluster, only ONE of the
   ## urls will be written to each interval.
   ##   ex: urls = ["https://us-west-2-1.aws.cloud2.influxdata.com"]
   urls = ["${url}"]

   ## Local address to bind when connecting to the server
   ## If empty or not set, the local address is automatically chosen.
   # local_address = ""

   ## Token for authentication.
   token = "${token}"

   ## Organization is the name of the organization you wish to write to.
   organization = "${org}"

   ## Destination bucket to write into.
   bucket = "${bucket}"

   ## The value of this tag will be used to determine the bucket.  If this
   ## tag is not set the 'bucket' option is used as the default.
   # bucket_tag = ""

   ## If true, the bucket tag will not be added to the metric.
   # exclude_bucket_tag = false

   ## Timeout for HTTP messages.
   # timeout = "5s"

   ## Additional HTTP headers
   # http_headers = {"X-Special-Header" = "Special-Value"}

   ## HTTP Proxy override, if unset values the standard proxy environment
   ## variables are consulted to determine which proxy, if any, should be used.
   # http_proxy = "http://corporate.proxy:3128"

   ## HTTP User-Agent
   # user_agent = "telegraf"

   ## Content-Encoding for write request body, can be set to "gzip" to
   ## compress body or "identity" to apply no encoding.
   # content_encoding = "gzip"

   ## Enable or disable uint support for writing uints influxdb 2.0.
   # influx_uint_support = false

   ## When true, Telegraf will omit the timestamp on data to allow InfluxDB
   ## to set the timestamp of the data during ingestion. This is generally NOT
   ## what you want as it can lead to data points captured at different times
   ## getting omitted due to similar data.
   # influx_omit_timestamp = false

   ## HTTP/2 Timeouts
   ## The following values control the HTTP/2 client's timeouts. These settings
   ## are generally not required unless a user is seeing issues with client
   ## disconnects. If a user does see issues, then it is suggested to set these
   ## values to "15s" for ping timeout and "30s" for read idle timeout and
   ## retry.
   ##
   ## Note that the timer for read_idle_timeout begins at the end of the last
   ## successful write and not at the beginning of the next write.
   # ping_timeout = "0s"
   # read_idle_timeout = "0s"

   ## Optional TLS Config for use on HTTP connections.
   # tls_ca = "/etc/telegraf/ca.pem"
   # tls_cert = "/etc/telegraf/cert.pem"
   # tls_key = "/etc/telegraf/key.pem"
   ## Use TLS but skip chain & host verification
   # insecure_skip_verify = false

   ## Rate limits for sending data (disabled by default)
   ## Available, uncompressed payload size e.g. "5MB"
   # rate_limit = "unlimited"
   ## Fixed time-window for the available payload size e.g. "5m"
   # rate_limit_period = "0s"



 # Send telegraf metrics to file(s)
 #[[outputs.file]]
 #  ## Files to write to, "stdout" is a specially handled file.
 #  files = ["output"]

   ## Use batch serialization format instead of line based delimiting.  The
   ## batch format allows for the production of non line based output formats and
   ## may more efficiently encode and write metrics.
   # use_batch_format = false

   ## The file will be rotated after the time interval specified.  When set
   ## to 0 no time based rotation is performed.
   # rotation_interval = "0h"

   ## The logfile will be rotated when it becomes larger than the specified
   ## size.  When set to 0 no size based rotation is performed.
   # rotation_max_size = "0MB"

   ## Maximum number of rotated archives to keep, any older logs are deleted.
   ## If set to -1, no archives are removed.
   # rotation_max_archives = 5

   ## Data format to output.
   ## Each data format has its own unique set of configuration options, read
   ## more about them here:
   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_OUTPUT.md
   # data_format = "influx"

   ## Compress output data with the specified algorithm.
   ## If empty, compression will be disabled and files will be plain text.
   ## Supported algorithms are "zstd", "gzip" and "zlib".
   # compression_algorithm = ""

   ## Compression level for the algorithm above.
   ## Please note that different algorithms support different levels:
   ##   zstd  -- supports levels 1, 3, 7 and 11.
   ##   gzip -- supports levels 0, 1 and 9.
   ##   zlib -- supports levels 0, 1, and 9.
   ## By default the default compression level for each algorithm is used.
   # compression_level = -1


###############################################################################
#                            PROCESSOR PLUGINS                                #
###############################################################################


# # Performs file path manipulations on tags and fields
 [[processors.filepath]]
#   ## Treat the tag value as a path and convert it to its last element, storing the result in a new tag
#   [[processors.filepath.basename]]
#     tag = "path"
#     dest = "basepath"
#
#   ## Treat the field value as a path and keep all but the last element of path, typically the path's directory
#   # [[processors.filepath.dirname]]
#   #   field = "path"
#
# Treat the tag value as a path, converting it to its the last element without its suffix
 [[processors.filepath.stem]]
   tag = "path"
#
# Treat the tag value as a path, converting it to the shortest path name equivalent
# to path by purely lexical processing
 [[processors.filepath.clean]]
   tag = "path"
#
#   ## Treat the tag value as a path, converting it to a relative path that is lexically
#   ## equivalent to the source path when joined to 'base_path'
#   # [[processors.filepath.rel]]
#   #   tag = "path"
#   #   base_path = "/var/log"
#
#   ## Treat the tag value as a path, replacing each separator character in path with a '/' character. Has only
#   ## effect on Windows
#   # [[processors.filepath.toslash]]
#   #   tag = "path"

 [[processors.converter]]
  [processors.converter.fields]
    float = ["CallRate", "TargetRate"]



###############################################################################
#                            INPUT PLUGINS                                    #
###############################################################################


 # Parse the new lines appended to a file
 [[inputs.tail]]
   ## File names or a pattern to tail.
   ## These accept standard unix glob matching rules, but with the addition of
   ## ** as a "super asterisk". ie:
   ##   "/var/log/**.log"  -> recursively find all .log files in /var/log
   ##   "/var/log/*/*.log" -> find all .log files with a parent dir in /var/log
   ##   "/var/log/apache.log" -> just tail the apache log file
   ##   "/var/log/log[!1-2]*  -> tail files without 1-2
   ##   "/var/log/log[^1-2]*  -> identical behavior as above
   ## See https://github.com/gobwas/glob for more examples
   ##
   files = ["${logspath}/*_stats.csv"]
   name_override = "stats"
   #path_tag = ""

   ## Offset to start reading at
   ## The following methods are available:
   ##   beginning          -- start reading from the beginning of the file ignoring any persisted offset
   ##   end                -- start reading from the end of the file ignoring any persisted offset
   ##   saved-or-beginning -- use the persisted offset of the file or, if no offset persisted, start from the beginning of the file
   ##   saved-or-end       -- use the persisted offset of the file or, if no offset persisted, start from the end of the file
   # initial_read_offset = "saved-or-end"

   ## Whether file is a named pipe
   # pipe = false

   ## Method used to watch for file updates.  Can be either "inotify" or "poll".
   ## inotify is supported on linux, *bsd, and macOS, while Windows requires
   ## using poll. Poll checks for changes every 250ms.
   # watch_method = "inotify"

   ## Maximum lines of the file to process that have not yet be written by the
   ## output.  For best throughput set based on the number of metrics on each
   ## line and the size of the output's metric_batch_size.
   # max_undelivered_lines = 1000

   ## Character encoding to use when interpreting the file contents.  Invalid
   ## characters are replaced using the unicode replacement character.  When set
   ## to the empty string the data is not decoded to text.
   ##   ex: character_encoding = "utf-8"
   ##       character_encoding = "utf-16le"
   ##       character_encoding = "utf-16be"
   ##       character_encoding = ""
   # character_encoding = ""

   ## Data format to consume.
   ## Each data format has its own unique set of configuration options, read
   ## more about them here:
   ## https://github.com/influxdata/telegraf/blob/master/docs/DATA_FORMATS_INPUT.md
   data_format = "csv"
   csv_header_row_count = 1
   csv_delimiter = ";"
   csv_trim_space = true
   csv_skip_values = [""]
   csv_timestamp_column = "CurrentTime"
   csv_timestamp_format = "2006-01-02T15:04:05.999999Z07:00"
   #csv_timezone = "Europe/Athens"

   ## Set the tag that will contain the path of the tailed file. If you don't want this tag, set it to an empty string.
   # path_tag = "path"

   ## Filters to apply to files before generating metrics
   ## "ansi_color" removes ANSI colors
   # filters = []

 [[processors.regex]]
   ## Rename metric fields
   [[processors.regex.field_rename]]
     pattern = "(SuccessfulCall|FailedCall)\\(C\\)"
     replacement = "Cum_${1}"
   [[processors.regex.field_rename]]
     ## Regular expression to match on the field name
     pattern = ".*\\(C\\)"
     ## Replacement expression defining the name of the new field
     replacement = ""
     ## If the new field name already exists, you can either "overwrite" the
     ## existing one with the value of the renamed field OR you can "keep"
     ## both the existing and source field.
     result_key = "overwrite"
   [[processors.regex.field_rename]]
     ## Regular expression to match on the field name
     pattern = "(.*)\\(P\\)"
     ## Replacement expression defining the name of the new field
     replacement = "${1}"
   [[processors.regex.tags]]
     ## Tag(s) to process with optional glob expressions such as '*'.
     key = "path"
     ## Regular expression to match the tag value. If the value doesn't
     ## match the tag is ignored.
     pattern = "^(.*)_stats"
     ## Replacement expression defining the value of the target tag. You can
     ## use regexp groups or named groups e.g. ${1} references the first group.
     replacement = "${1}"
     ## Name of the target tag defaulting to 'key' if not specified.
     ## In case of wildcards being used in `key` the currently processed
     ## tag-name is used as target.
     # result_key = "method"
     ## Appends the replacement to the target tag instead of overwriting it when
     ## set to true.
     # append = false
   #[[processors.regex.fields]]
   #  ## Field(s) to process with optional glob expressions such as '*'.
   #  key = "CurrentTime"
   #  ## Regular expression to match the field value. If the value doesn't
   #  ## match or the field doesn't contain a string the field is ignored.
   #  pattern = "(.*)[+\\-Z].*"
   #  ## Replacement expression defining the value of the target field. You can
   #  ## use regexp groups or named groups e.g. ${method} references the group
   #  ## named "method".
   #  replacement = "${1}"
   #  ## Name of the target field defaulting to 'key' if not specified.
   #  ## In case of wildcards being used in `key` the currently processed
   #  ## field-name is used as target.
   #  #result_key = "_ignore_"


 # Process metrics using a Starlark script
 [[processors.starlark]]
   ## The Starlark source can be set as a string in this configuration file, or
   ## by referencing a file containing the script.  Only one source or script
   ## should be set at once.

   ## Source of the Starlark script.
   source = '''
def apply(metric):
    for field, value in list(metric.fields.items()):
        if value == 0:  # Check if field value is 0i
            metric.fields.pop(field)  # Remove the field
    return metric
 '''

